# Lesson 04

* [Download the HTML file for this lesson](/lessons/04.html?raw=true)
* [Previous lesson](/lessons/03.md)
* [Next lesson](/lessons/05.md)
* [Back to lesson listing](/lessons/README.md)

This lesson is about what happens when a fragment (i.e. pixel) is covered by more than one shape.

As you've seen in Lessons 01 through 03, WebGL requires a bit of boilerplate code just to get
started, and it'll always be pretty much the same. Usually you would write some helper functions and
put them in a separate script, so you don't need to include the boilerplate code every time. For
educational purposes, though, I'm going to leave it in the html, in a separate `script` tag with an
id of `boilerplate`. You can skip over the boilerplate code when reading this and future lessons,
but it's there if you need to remind yourself how something is done.

Background
==========

In 2D graphics, in order to draw a scene, you sort your objects by z-index (i.e. how far away they
are from the viewer) and just draw them one at a time, on top of each other. In 3D graphics, this
is, in general, much harder, because you need to deal with complex objects in many different
orientations, and the objects are often made with many different shapes, and you don't know their
position until after you've run the vertex shader. So sorting them by z-index before issuing the
draw commands is not an option.

If you recall, it's possible to set the depth (equivalent to z-index) in the vertex shader with
the z-component of `gl_Position`, so WebGL is aware of a fragment's depth. It would be nice if there
was a way for fragments that come out of the fragment shader to be automatically sorted by depth
before being applied to the framebuffer. But for whatever reason, this is not possible. The
fragments are applied in the order they're called. So you must rely on another tool: depth testing.

The depth buffer
===============

In a framebuffer (such as the viewport), there are three kinds of buffers: the color buffer, the
depth buffer, and the stencil buffer. (The stencil buffer is useful for a few effects like portals
and shadows, but it's kind of advanced, so I won't be covering it.) Each buffer is a chunk of memory
containing values for each fragment in the framebuffer. The color buffer is easy to understand. It
contains the rgba values of the colors you actually see in the viewport. You don't view the contents
of the depth buffer directly, but it's there too.

So what does the depth buffer contain, and how does it help? It contains a value from 0 to 1 for
each fragment, that represents the distance from the viewer of the surface covering that fragment.
0 is nearest and 1 is farthest. The depth buffer is cleared at the same time as the color buffer if
you specify both when you call:

	gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT)

The value that it's cleared to can be set with `gl.clearDepth`, but the default value of 1 is almost
always what you want.

Standard depth testing
======================

When a new surface is being written to a fragment, WebGL performs depth testing, and only updates
the fragment if the new depth is less than the old depth (i.e., the new object is closer to the
viewer than the existing object). If the new object is farther from the viewer than the object
that's already there, the new object is simply not drawn. If it is drawn, the depth buffer is also
updated with the new object's depth value, which is how WebGL knows the depth of the object that's
currently drawn to the fragment.

An object's depth is determined from the z-component of `gl_Position`. If you remember how clip
space works, objects will pass the clip test and go on to the fragment shader if the z-component of
their `gl_Position` is between -1 and 1. This z-component will then be mapped to the range [0, 1],
which is used as the object's depth to see if it passes the depth test. (You can set the range it's
mapped to with `gl.depthRange`, but this is rarely useful: [0, 1] is usually what you want.)

That's the standard behavior, anyway. There are a number of settings that affect exactly whether and
how depth testing happens. To get the standard behavior, use the following:

	gl.enable(gl.DEPTH_TEST)
	gl.depthFunc(gl.LESS)  // Optional (it's the default)

The depth function is the criterion that a fragment must have in order to pass the depth test and be
drawn to the viewport. `gl.LESS` means it passes the depth test if the z-component of its
`gl_Position` value (and therefore its depth) is less than what's already written to the depth
buffer. There are other options like `gl.GREATER` that are used rarely for certain special effects.

It's also possible to keep depth testing on, but not update the depth buffer when a new fragment is
added, by turning off the depth mask:

	gl.depthMask(false)

This may be useful in rare situations.

Blending
========

So, if a fragment passes the depth test and gets written to the framebuffer, whatever was there
before is thrown out, right? Not necessarily. Blending is the process of taking into account both
the current value in the color buffer (destination) and the incoming value (source) to get the new
value. It's how WebGL implements semi-transparent surfaces. Blending is a complicated subject, so if
you have the luxury of only having solid, opaque objects, just skip ahead to general recommendations
below.

Blending is off by default, so if you have any transparency in your scene, enable it with:

	gl.enable(gl.BLEND)

There are a huge number of possible blending functions that take into account the alpha value of the
source and destination fragments. They're set through `gl.blendFunc` and `gl.blendEquation`, but I
won't go into details. This is the one you want:

	gl.blendFuncSeparate(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA, 0, 1)

This is a WebGL-specific recommendation. Other OpenGL tutorials will recommend a combined blending
function rather than this separate one. (If you want to know the reason for this distinction, it's
because of the alpha channel of the canvas itself. Other OpenGL programs run in their own window,
whereas WebGL runs in a web page. So if a WebGL framebuffer winds up being transparent, the page
background will show through. The separated blending function above ensures that if you set the
alpha channel to 1 initially when you call `gl.clear`, it will always remain 1 no matter what else
you draw on it.)

Anyway, the line above corresponds to the blending function:

	red(new) = red(source) x alpha(source) + red(destination) x (1 - alpha(source))

and similarly for the green and blue channels. This is the intuitive meaning of the alpha channel
that most people have. Again, some unusual effects call for a different formula.

Although I don't recommend it, one alternative is using the non-separated blending function that
most OpenGL tutorials use, and disabling the alpha color mask:

	gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA)
	gl.colorMask(true, true, true, false)

This will prevent any new fragment from updating the alpha values in the color buffer. The only
thing is that you need to have it enabled when you call `gl.clear` the first time, since the alpha
values default to `0.0`.

Depth and blending options
==========================

`04.html` shows several different possible options of depth testing, blending, and depth mask. Each
of the eight columns shows a different combination of options. Shapes are drawn with these options
with various alpha transparency values.

The standard WebGL options are used in the fifth column:

	gl.enable(gl.DEPTH_TEST)
	gl.depthFunc(gl.LESS)
	gl.enable(gl.BLEND)
	gl.blendFuncSeparate(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA, 0, 1)

This is what you almost always want. But, if you look closely, you can see it's not perfect. It
shows exactly what you want for the solid opaque points at the top, with red in front and yellow and
blue in back. But look at the transparent squares below. The white stripe is visible through roughly
as you expect, but you should also be able to see some of the yellow behind the red, and some of the
blue behind the red and yellow. It gets worse as you go down. By the time you get the bottom with
25% opacity, you really ought to be able to see some of the background colors. In my opinion,
columns 6 and 8, although quite wrong on the top row, look better on the bottom row.

This problem would go away if we had properly sorted the surfaces by depth, and drawn the farthest
point (blue) first, and the nearest (red) last. Then several of the columns, including column 5,
would look exactly right.

General recommendations
=======================

So what's the solution? There's no perfect one, but here are my general recommendations:

If you don't have any alpha transparency in your scene, great. Use the standard depth settings, and
you can disable blending altogether. You're done, and you don't need to sort your surfaces.

	gl.enable(gl.DEPTH_TEST)
	gl.depthFunc(gl.LESS)
	gl.disable(gl.BLEND)

Otherwise use the standard depth and blending settings.

Try to separate out your solid and transparent surfaces. Draw your solid ones first and your
transparent ones last.

If you can sort your transparent surfaces by depth, it's probably worth it. If you have transparent
surfaces that you can't sort by depth, use the standard settings and try disabling the depth mask
(column 8) only while drawing transparent surfaces.

	gl.depthMask(false)

Use your judgment as to whether that improves the scene.

Getters used in this lesson
===========================

	// Boolean: whether depth testing is enabled
	gl.getParameter(gl.DEPTH_TEST)
	// Number: number of bits in your depth buffer (at least 16)
	gl.getParameter(gl.DEPTH_BITS)
	// Array of 2 Numbers: range of the depth buffer to write to. Defaults to [0, 1]
	gl.getParameter(gl.DEPTH_RANGE)
	// Boolean: whether the depth buffer is writeable
	gl.getParameter(gl.DEPTH_WRITEMASK)
	// Number: value that the depth buffer will be cleared to. Defaults to 1.
	gl.getParameter(gl.DEPTH_CLEAR_VALUE)
	// GLenum: current setting of gl.depthFunc, e.g. gl.LESS.
	gl.getParameter(gl.DEPTH_FUNC)

Exercises
=========

1. Change `drawpoints` so that the yellow point appers on top when depth testing is enabled in
column 4, but on bottom when depth testing is not enabled in column 3. You'll need to rearrange the
draw order of the points, and change the z-coordinate of the yellow point.
1. Change `drawcolumn` to make the bottom row always have an alpha value of 0. Guess before doing
this what the bottom row will look like in each column.
1. Change `drawpoints` to make the blue point always be solid (alpha = 1). Guess what the bottom
row will look like in each column.
1. Change the depth func to `gl.GREATER` instead of `gl.LESS`. Again, guess what will happen.
1. Before any drawing commands are issued, use `gl.clearDepth(0.65)` to change the clear depth, and
then call `gl.clear` on the depth buffer. Since this depth is closer than the blue points, guess
which columns this will affect the blue points in.
