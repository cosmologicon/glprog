# Lesson 05

Lessons 05, 06, and 07 are a crash course on GLSL ES, the language of shaders in WebGL. We'll build
a 3d scene of balls arranged in a grid, and in doing so use a lot of GLSL features.

If you're using other references, keep in mind that other versions of OpenGL use other versions of
GLSL, and many GLSL versions contain features that GLSL ES lacks.

This lesson is where we start really using linear algebra, specifically vectors and matrices. I'm
not going to try to teach you the basics of linear algebra while also teaching you WebGL. If you're
not comfortable with algebra yet, you really need to get a textbook or take an online course or
something before working with 3d graphics.

## Basic syntax

I won't give a complete description of GLSL, but it's a curly-brace language, with syntax generally
similar to C, C++, Java, and JavaScript. `if` and `else` function the same as in JavaScript.
Assignment works the same. Comments work the same. Arithmetic, bitwise, and logical operators work
the same. Variable names are case sensitive. Statements end in semicolons and variables need type
declarations, like in C. There are, of course, many differences between GLSL and other languages,
but if in doubt, try the syntax you're used to.

## GLSL variable types

GLSL is strongly typed. Here's a complete list of the types in GLSL ES:

* booleans (`bool`) and boolean vectors (`bvec2`, `bvec3`, `bvec4`)
* integers (`int`) and integer vectors (`ivec2`, `ivec3`, `ivec4`)
* floating point values (`float`) and float vectors (`vec2`, `vec3`, `vec4`)
* floating point square matrices (`mat2`, `mat3`, `mat4`)
* sampler types (`sampler2D`, `samplerCube`)
* 1-dimensional arrays of any of the above values or data structures
* data structures (`struct`) incorporating the above values or other structures

Other GLSL versions have other types, such as non-square matrices and 3D samplers, but this is all
there is for GLSL ES.

Booleans, integers, and floats should be pretty familiar if you're used to other languages.

We'll see vectors in this lesson, and matrices in Lessons 06 and 07. In some languages like C++,
"vector" means something like a JavaScript Array. But that's not what "vector" means here: a vector
in GLSL has a fixed number of elements (2, 3, or 4). It's more like a vector in mathematics. We've
already seen that both position and color are represented by vectors, so expect to use them a lot.

Sampler types are how you access textures (images). We'll see them in Lessons 08 and 09.

Data structures are similar to structs in C. Arrays are different from JavaScript Arrays. They're
essentially a kind of data structure, with N different data memebers of the same type. There are
some significant limitations on how you can index them. Data structures and arrays are useful for
organizing large shaders. However, they can be complicated to use, and they're never essential. As
the shaders we'll be working with will be relatively small, I will not be using them.

## Precision

GLSL variables (other than boolean variables) have a precision, which is one of `highp`, `mediump`,
or `lowp`. This is separate from their type. A `mediump int` and a `highp int` are the same type,
and there is automatic conversion between them, unlike between `float` and `int`.

The WebGL specification doesn't say how precise these precisions are. It only sets required minimum
values, and the minimums are pretty low. The required precisions for low, medium, and high precision
values are 8, 10, and 16 bits. For comparison, IEEE single-precision floats have 23 bits of
precision, and double-precision floats (which is the format for JavaScript's Number type) have 52
bits of precision. The precision can also be different in vertex shaders and fragment shaders, as
long as each of them meets the minimum requirements. Furthermore, only vertex shaders are required
to support `highp`. Fragment shaders are only required to support `lowp` and `mediump`, with `highp`
being optional.

Having said that, most implementations exceed the requirements, especially for desktop browsers: the
specification "highly recommends" that `highp` have as much precision as single-precision floats.
I just checked in Chrome, and all three float precisions (even `lowp`) are equivalent to
single-precision floats, and all three integer precisions are equivalent to 32-bit integers, in both
kinds of shaders. Anyway, practically speaking, relatively low precisions are fine for shader math.

So which precision should you choose, especially if you don't know how precise they actually are?
The standard practice seems to be to use `highp` in vertex shaders, and `mediump` in fragment
shaders. You would only go below this if you really need to optimize, you know what you're doing,
and you're willing to test on a wide variety of platforms. (One exception: if you declare and use a
uniform variable in both shaders, the precision has to match, so you would probably choose `mediump`
in both shaders.)

This standard practice is also the built-in default, at least for both floats and ints in vertex
shaders, and ints in fragment shaders. Floats in fragment shaders do not have a built-in default,
and so they must either have a precision declared, or have a default precision declared within their
shader. You can set a default precision for a shader with the `precision` keyword.

	precision highp float;
	precision mediump int;

You can use the `precision` keyword to change the default precision multiple times throughout the
shader, and set a different default precision within a function or block. Usually, though, you just
set it once at the top. So the standard practice is to place `precision mediump float;` at the top
of your fragment shader.

Sampler types also have a precision, which defaults to `lowp`, and they can have a default precision
set. But `lowp` is usually fine for samplers. We'll cover them in Lesson 08.

## `gl_FragColor`

Let's start with the fragment shader. As we saw in Lesson 03, the purpose of the fragment shader is
to determine the color that should be drawn at this fragment (which is the same thing as a pixel),
and to write that color value to the global variable `gl_FragColor`, which is a `mediump vec4` of
rgba values between 0 and 1.

There's another, similar, output from the fragment shader, called `gl_FragData`. However, it's
useless without certain WebGL extensions. Your WebGL implementation doesn't necessarily support
these extensions (though they are fairly common), and it's a pretty advanced feature, so I won't be
discussing `gl_FragData`.

In this program, we're going to draw a 5x5x5 cube of 125 points, each with a different color. Let's
number the points with an `ivec3` (vector of 3 integers) called `index`, ranging from `(-2, -2, -2)`
in one corner to `(2, 2, 2)` in the opposite corner. The point in the center has coordinates
`(0, 0, 0)`. Now we can assign a different color to each point by calculating a color value in terms
of `index`:

	vec3 color = 0.5 + vec3(index) / 6.0;

Remember that in GLSL, there's no automatic conversion from ints to floats, so we have to cast from
`ivec3` to `vec3` explicitly here. Since the components of `index` range from `-2` to `2`, this
formula means the components of `color` will range from 1/6 (dark) to 5/6 (light). For now let's
just write this to `gl_FragColor`, which will make each point a solid square of color:

	gl_FragColor = vec4(color, 1.0);

In lessons 06 and 07 we'll change this up to produce lighting and shape effects.

## Vertex shader

Now let's look at the vertex shader. The vertex shader runs once, in parallel, for each vertex. When
you're drawing points like we're doing, the vertex is the center of the point. Since we're only
drawing one vertex at a time, there's nothing to parallelize here, but typically you would draw all
125 points with a single draw command. We'll see how to do that in Lesson 11.

The purpose of the vertex shader is to assign a value to the global `highp vec4 gl_Position`, which
is the vertex's position in clip space. If you're drawing points, it can optionally also assign a
value to the global `highp float gl_PointSize`, which is the size of the point in pixels.

Typically you start with the position of your objects in some logical "world" or "game" coordinate
system. You then transform this game position into a position with respect to the camera, which you
then transform into a position on the screen, which you finally need to transform to position in
clip space coordinates so that it can be assigned to `gl_Position`. So let's start by simply
defining the game position to be equal to `index`, which is the same `ivec3` uniform from the
fragment shader, running from `(-2, -2, -2)` to `(2, 2, 2)`:

	vec3 gamepos = vec3(index);

Our cube is 4x4x4 game units (if you count the distance between point centers), so I figure making
the viewport be 8 game units high should be about right. If we want to transform `gamepos` into
viewport coordinates, we need the number of pixels per game unit:

	// Pixels per game unit.
	const float gamescale = viewportsize.y / 8.0;
	vec3 viewpos = viewportsize * 0.5 + gamepos * gamescale;

We add `viewportsize * 0.5`, because `gamepos` is equal to `(0, 0, 0)` at the center of the
viewport, but viewport coordinates run from `(0, 0)` on the bottom left to `(854, 480)` on the top
right. Now we need to convert viewport coordinates to clip space coordinates:

	vec3 clippos = viewpos / viewportsize * 2.0 - 1.0;

Both game coordinates and clip coordinates have `(0, 0)` at the center, so you may notice that we
transformed away from having the origin at the center just to cancel it out. That's true, but it
won't always be like that.

## The z coordinate

Notice that everything we did used 3-dimensional vectors. Where's the z coordinate in all of this?
Conceptually, every point's z coordinate tells you how far it is away from the viewer (even though,
of course, in the end everything gets rendered to a flat surface). Points with `gamepos.z` equal to
`-2` are closest to the viewer, and ones with `gamepos.z` equal to `2` are farthest away.
Practically, though, `gl_Position.z` doesn't affect where the point appears on the screen, so does
it matter?

For one thing, it matters for depth testing, as we saw in Lesson 04. If depth testing is enabled and
two points are drawn to the same place on the screen, the one with the lower `gl_Position.z` will be
the one that's visible.

For another thing, clip space coordinates must all be between -1 and 1 for the point to be drawn.
For the x and y clip space coordinates, this is intuitive, because if they fall outside this range
then they would be outside the viewport. But the same restriction applies to z coordinates. If a
point is too close (`gl_Position.z < -1`) or too far away (`gl_Position.z > 1`) then it won't be
rendered.

For another thing, the fragment shader is aware of the point's depth, and as we'll see in Lesson 07,
it's perfectly possible to change a color based on it.

Finally, remember that even though `gamepos.z` doesn't affect the point's position on the screen in
this lesson, that won't always be the case. If the camera is rotating around the object like we'll
see in Lesson 06, then the game space z-axis could be completely different from the clip space
z-axis. So yeah, it's important to treat the z coordinate properly.

## Optimization

Now, if you remember this from algebra, you might realize that we just did a bunch of affine
transformations, to get from game coordinates to clip coordinates, and that these can be combined
into a single affine transformation. If you work out the algebra, you'll see that this:

	vec3 viewportsize = vec3(854.0, 480.0, 854.0);
	float gamescale = viewportsize.y / 8.0;
	vec3 viewpos = viewportsize * 0.5 + gamepos * gamescale;
	vec3 clippos = viewpos / viewportsize * 2.0 - 1.0;
	gl_Position = vec4(clippos, 1.0);

Is the same as this:

	const vec3 transform = vec3(0.140625, 0.25, 0.140625);
	gl_Position = vec4(transform * gamepos, 1.0);

which involves fewer operations, and might lead to better performance (thought with GLSL compiler
optimizations, you shouldn't assume). Of course, the second version is much harder to understand,
and much harder to update. What happens if you change the viewport size?

The common way to handle this is to have a well-documented function on the JavaScript side that
combines all the different transformations and produces the `transform` vector, then passes it as a
uniform into the shader. (In general it'll be a transformation matrix, not a vector, but the
principle is the same.) That way you can just update the arguments to the function when you need to,
and the shader can remain short.

But since this is a lesson, we'll stick with the way that's simpler to understand.

## Exercises

1. Add the line `gamepos.xy += 0.1 * gamepos.z;` so that points at different depths are offset from
each other on the screen.
1. Change the precision of the uniform `index` from `mediump` to `highp`. Remember that it needs to
match in both shaders.
1. Change the value of `viewportsize` in the vertex shader to `vec3(854.0, 480.0, 100.0)` (i.e.
change `viewportsize.z` from 854 to 100). Reload the page and explain what you see.
1. Add the following line after the definition of `gamepos`: `gamepos = normalize(gamepos);` This
line normalizes the position vector, so that every point will be 1 unit from the center. What shape
do you expect the points to make?
1. Change the formula that converts `index` to `color` in the fragment shader, so that the center
point of the grid has the darkest color, and the corners have the brightest colors.

